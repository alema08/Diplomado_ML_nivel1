{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sesion15.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZTrlCzdxYsxx"},"source":["<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://upload.wikimedia.org/wikipedia/commons/archive/f/fb/20161010213812%21Escudo-UdeA.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n","\n","# **Diplomado de Análisis de datos y Machine Learning en Python**\n","\n","\n","El presente diplomado hace parte del centro de Big Data de la facultad de ciencias exactas y naturales (FCEN) de la Universidad de Antioquia.\n","\n","## **Sesión 15**\n","\n","## **Contenido**\n","\n","- <a href=\"#pip\"> Pipelines</a><br>\n","- <a href=\"#val\"> Validación cruzada</a><br>\n","- <a href=\"#pol\"> Regresión polinomial</a><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BQqJQ-MmYy3s"},"source":["<p><a name=\"pip\"></a></p>\n","\n","# **Pipelines**\n","\n","Hemos visto cómo la aplicación de un sistema de ML requiere de diferentes etapas: preprocesado de los datos, construcción del modelo y su posterior evaluación. Vimos también cómo esta etapa de preprocesado afecta directamente los resultados que obtenemos del modelo final.\n","\n","Vamos a ver cómo podemos simplificar los diferentes pasos en el preprocesamiento, así como la construcción del modelo, mediante el uso de los *pipelines*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F3O3d6HzhFqt"},"source":["<p><img alt=\"Colaboratory logo\" height=\"340px\" src=\"https://i.imgur.com/Q5C8NGb.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>"]},{"cell_type":"markdown","metadata":{"id":"p6cKkHA5hmnI"},"source":["Un *Pipeline* es un objeto que nos permite encadenar varios pasos del workflow de ML secuencialmente. Específicamente, un Pipeline agrupa los pasos de preprocesamiento y modelado como si tratara de un solo paso, de manera que podamos automatizar todo el proceso manteniendo el código de preprocesamiento y de modelado de una forma muy organizada.\n","\n","Las dos principales ventajas de trabajar con pipelines son las siguientes:\n","\n","* Normalmente necesitaremos realizar un seguimiento como tal de los datos de entrenamiento y prueba en cada uno de los pasos de preprocesado, modelado y evaluación, lo que supone una tarea muy complicada desde el punto de vista de la implementación. Con los *pipelines* no necesitaremos de este seguimiento y tendremos un código mucho más simple y limpio.\n","\n","* Con los pipelines, vamos a poder evaluar todo el proceso, que incluye las etapas de preprocemiento y de construcción del modelo, como un todo. Más que evaluar solo el modelo final, como lo veníamos haciendo.\n","\n","Tal vez esta última sea la principal ventaja que tendremos a la hora de trabajar con pipelines.\n","\n","Volvamos al problema en el que hemos estado trabajando:"]},{"cell_type":"code","metadata":{"id":"etvISNF9Ym8g"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lEfwWhvLi_Le"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4o7VKvsyjGtc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0FmZIZNjyJp"},"source":["El *pipeline* aplica secuencialmente una lista de transformadores y un estimador final. Los pasos intermedios del *pipeline* deben ser 'transformaciones', es decir, deben implementar los métodos `fit` y `transform`. El estimador final solo necesita implementar el método `fit`."]},{"cell_type":"code","metadata":{"id":"lb4rOJlujkcf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E62ktLTLlRa1"},"source":["Variables numéricas:"]},{"cell_type":"code","metadata":{"id":"wzAevJQwYsXF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSfnaBpPkPp9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGpxtCWJltvx"},"source":["Variables categóricas:"]},{"cell_type":"code","metadata":{"id":"dSEU8c2tltBY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLU7oA9sm0mG"},"source":["Alternativamente, podemos utilizar la clase ` make_pipeline`, la cual es un alias para `pipeline` en la que no tenemos que hacer explítico el nombre de la transformación:"]},{"cell_type":"code","metadata":{"id":"h2SQ-4uCnA0A"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8Ql43KEnUd5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rP6uWw1RmSF6"},"source":["Ahora, vamos a querer aplicar estas transformaciones que hemos definido a nuestro conjunto de datos. \n","\n","En principio podríamos seleccionar las caracteristicas numéricas y categóricas y aplicar la transformación correspondiente a cada una de estas y al final combinar ambas características transformadas en un solo conjunto de datos. \n","\n","Sin embargo, este es el proceso que queremos evitar, recordemos que queremos tener todo el proceso automatizado, de manera que no nos tengamos que preocupar por este tipo de pasos. \n","\n","Lo que haremos, será utilizar un transformador de columnas `ColumnTransformer` (o `make_column_transformer`), de manera que construyamos un único objeto que realice estas tareas de preprocesamiento de una forma más simple y automática."]},{"cell_type":"code","metadata":{"id":"RbIVB4wup-J5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3R2R3yLbntJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLhoRtLcmMZK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLavWjY3ojcw"},"source":["Ahora que tenemos el proceso de transformación de los datos en este objeto, nos resta añadir el paso de construcción del modelo de manera que tengamos todo el proceso en un solo objeto. Para esto, de nuevo, utilizaremos un Pipeline"]},{"cell_type":"code","metadata":{"id":"B9JDqzANml_g"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5K6k3ULKoqJo"},"source":["Una vez tenemos todo el proceso en este único objeto, podemos entrenar el modelo"]},{"cell_type":"code","metadata":{"id":"FdgSS3L_otFH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaHl5D-3rZQB"},"source":["Evaluemos el modelo"]},{"cell_type":"code","metadata":{"id":"3hx8aOTpqkrm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtKRc2_xdNN_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQusunhdo4OP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qd-DC-fbp2n8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fUP5wlxHr4eO"},"source":["<p><a name=\"val\"></a></p>\n","\n","# **Validación cruzada**\n","\n","Hasta el momento lo que hemos hecho para la evaluación de los modelos supervisados es lo siguiente:\n","\n","* Separar nuestro conjunto de datos en conjuntos de entrenamiento y prueba usando el método `train_test_split`.\n","* Entrenar un modelo a partir de los datos de entrenamiento mediante el método `fit`. \n","* Evaluar con los datos de prueba usando el método `score`:\n","\n","Recuerdomes que la razón por la que dividimos nuestros datos en los conjuntos de entrenamiento y prueba, es que estamos interesados en medir qué tan bien nuestro modelo se generaliza a nuevos datos. Es decir, no nos interesa saber qué tan bien se ajusta nuestro modelo a los datos de entrenamiento, sino más bien qué tan bien puede hacer predicciones para datos que no se observaron durante el proceso de entrenamiento.\n","\n","Esta estrategia que hemos utilizado para la separación de los datos se conoce como *hold-out set*\n"]},{"cell_type":"markdown","metadata":{"id":"dqt_wUZZu2bj"},"source":["![picture](https://www.researchgate.net/profile/Brian_Mwandau/publication/325870973/figure/fig6/AS:639531594285060@1529487622235/Train-Test-Data-Split.png)"]},{"cell_type":"markdown","metadata":{"id":"RpaAmkFzvCI_"},"source":["Una desventaja de utilizar esta estrategia es que perdemos una porcion de nuestros datos para el entrenamiento del modelo. Esta no es la estrategia más óptima, y puede ademas causar problemas, especialmente si el conjunto de entrenamiento es pequeño.\n","\n","Una forma de resolver este problema es usando lo que se conoce como *validación cruzada* (VC)\n","\n","**K-fold Cross-validation**\n","\n","La VC es un método estadístico para evaluar el rendimiento de generalización de un modelo, que es más estable y completo.\n","\n","En la VC, se realiza una serie de ajustes donde cada subconjunto de los datos se utiliza tanto para el entrenamiento como para la prueba.\n","\n","La versión más utilizada de la validación cruzada es la validación cruzada k-fold.\n","\n","Por ejemplo, cuando utilizamos una VC de 5 folds, los datos se dividen en cinco partes de aproximadamente el mismo tamaño llamados folds, con los que se procede a entrenar una secuencia de modelos.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L1wmSmfdvsZG"},"source":["\n","\n","![picture](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/233459a6-523d-4cf7-91f3-ff539a1b58ce/f6c9980c-ed0d-4564-8289-e95c9274b48e/images/screenshot.png)\n","\n"]},{"cell_type":"code","metadata":{"id":"PXetoQh2eKkd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_LCep4qwLds"},"source":["Al final, habremos obtenido cinco puntajes de validación para nuestro modelo Entonces, cuando utilizamos VC, cada instancia del conjunto de datos estará en el conjunto de entrenamiento exactamente una vez.\n","\n","Es importante tener en cuenta que la validación cruzada no es una forma de construir un modelo que pueda aplicarse a nuevos datos. La validación cruzada no devuelve un modelo sino que crea múltiples modelos internamente con el propósito de evaluar qué tan bien se generalizará un modelo dado cuando se entrene en un conjunto de datos específico.\n","\n","Veamos este proceso explícitamente con 2 folds:\n"]},{"cell_type":"code","metadata":{"id":"_XUyCjKmxB7h"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-W0NG2yqnhB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFda0kzYxzGc"},"source":["La VC la podemos implementar directamente desde Sklearn con la clase `cross_val_score`:"]},{"cell_type":"code","metadata":{"id":"a3ze4jtvxtB7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRHCgMva3GzF"},"source":["Note que obtenemos un resultado diferente al caso anterior. \n","\n","Hagamos una validación con 5 folds:"]},{"cell_type":"code","metadata":{"id":"AkVVyciS0LDw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z9RhbHGu2sJ3"},"source":["Si observamos los cinco puntajes producidos por la validación cruzada, podemos concluir que existe una variación relativamente alta en la precisión entre los folds. Esto podría implicar que el modelo depende mucho de los folds particulares utilizados para el entrenamiento, pero también podría ser una consecuencia del pequeño tamaño del conjunto de datos.\n","\n","Para definir explícitamente la forma en que separaremos los datos, podemos utilizar la clase `KFold`"]},{"cell_type":"code","metadata":{"id":"4MYcgRon0s5L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JgNZUSGY4i0x"},"source":["En resumen\n","\n","* Con VC, utilizamos todos los datos para el entrenamiento, y no solo una porción de estos. cada instancia estará un uno de los folds, y cada fold a su vez será un conjunto de prueba una vez, por lo que estaremos usando nuestros datos de manera más efectiva. Así, el modelo necesita generalizar bien para todas las muestras del conjunto de datos de manera que todos los puntajes de validacion sean altos.\n","\n","* Tener múltiples divisiones de los datos también nos proporciona información sobre cuán sensible es nuestro modelo a la selección del conjunto de datos de entrenamiento. En el ejemplo que hicimos, vimos precisiones entre 76% y 90%. Este es un rango bastante amplio y nos proporciona una idea sobre cómo podría funcionar el modelo en el peor de los casos y en el mejor de los casos cuando se aplica a nuevos datos.\n","\n","* La principal desventaja de la validación cruzada es el aumento del costo computacional. Como ahora estamos entrenando k modelos en lugar de un solo modelo, la validación cruzada será aproximadamente k veces más lenta que hacer una sola división de los datos."]},{"cell_type":"markdown","metadata":{"id":"7qDIWnWV9iEI"},"source":["Ya hemos visto la receta básica para aplicar un modelo de ML supervisado:\n","\n","* Escoger un tipo de modelo\n","* Escoger unos hiperparámetros\n","* Entrenar el modelo con unos datos de entrenamiento\n","* Utilizar el modelo para obtener predicciones\n","\n","Surge una pregunta clave: ¿Si nuestro estimador tiene bajo rendimiento, qué deberíamos hacer? \n","\n","* Usar un modelo más complejo/flexible\n","* Usar un modelo menos complejo/flexible\n","* Conseguir más datos de entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"IEMMqL_q5ptm"},"source":["# **Regresión polinomial**\n","\n","<p><a name=\"pol\"></a></p>\n","\n","¿Qué pasa si nuestros datos tienen una relación no lineal?"]},{"cell_type":"code","metadata":{"id":"FwcNlPBI4dZH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RR55nvafB3og"},"source":["Un modelo lineal simple utiliza una combinación lineal de las características de entrada para predecir  una salida\n","\n","$$\\hat y = w_0 + w_1 x_1 + \\cdots + w_n x_n$$\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"t_F0s8v8_ZdQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ug4eQSLPFjlI"},"source":["Una forma fácil de extender el modelo lineal es incluir combinaciones de pares de características de entrada:\n","\n","$$\\hat y = w_0 + w_1 x_1 + \\cdots w_n x_n + w_{11}x_1 x_1 + w_{12} x_1 x_2 + \\cdots$$\n","\n","Esto nos permite capturar interacciones entre características y, por lo tanto, estos pares se denominan *características de interacción*. Si la transformación es de la forma $f_n(x) = x^n$ obtenemos lo que se conoce como una **regresión polinomial**. En nuestro caso, tendríamos:\n","\n","$$\\hat y = w_0 + w_1 x_1 + w_2 x_1^2$$\n","\n","De esta manera podemos adaptar la regresión lineal a relaciones no lineales entre las variables. Observe que este sigue siendo un modelo lineal: la linealidad se refiere al hecho de que los coeficientes nunca se multiplican ni se dividen entre sí. \n","\n","Lo que hemos hecho efectivamente es tomar nuestros valores de $X$ unidimensionales y proyectarlos en una dimensión superior, de modo que un ajuste lineal pueda encajar relaciones más complicadas entre $X$ y $y$.\n","\n","Podemos implementar la regresión polinomica utilizando el transformador `PolynomialFeatures` de Sklearn:"]},{"cell_type":"code","metadata":{"id":"1g0pAUsAGXAP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fBlMR88GshL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmAeXkGpL46N"},"source":["**Ejercicio:** Escriba una función que tome como argumento el grado del polinomio y que construya un Pipeline que encadene la transformación polinomial de las características y la construcción de un modelo de regresión lineal."]},{"cell_type":"code","metadata":{"id":"wifqIUjQK7Fz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLi_nO7ZL-dL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OH9W1sIt0n-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zx7HEBHwuYHD"},"source":[""],"execution_count":null,"outputs":[]}]}